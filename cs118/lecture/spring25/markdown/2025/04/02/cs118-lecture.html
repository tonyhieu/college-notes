<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Virtual Reality | Anthony Vo’s College Notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Virtual Reality" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="COMPSCI 118" />
<meta property="og:description" content="COMPSCI 118" />
<link rel="canonical" href="https://tonyhieu.github.io/college-notes/cs118/lecture/spring25/markdown/2025/04/02/cs118-lecture.html" />
<meta property="og:url" content="https://tonyhieu.github.io/college-notes/cs118/lecture/spring25/markdown/2025/04/02/cs118-lecture.html" />
<meta property="og:site_name" content="Anthony Vo’s College Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-02T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Virtual Reality" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-02T00:00:00-05:00","datePublished":"2025-04-02T00:00:00-05:00","description":"COMPSCI 118","headline":"Virtual Reality","mainEntityOfPage":{"@type":"WebPage","@id":"https://tonyhieu.github.io/college-notes/cs118/lecture/spring25/markdown/2025/04/02/cs118-lecture.html"},"url":"https://tonyhieu.github.io/college-notes/cs118/lecture/spring25/markdown/2025/04/02/cs118-lecture.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/college-notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://tonyhieu.github.io/college-notes/feed.xml" title="Anthony Vo's College Notes" /><link rel="shortcut icon" type="image/x-icon" href="/college-notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/college-notes/">Anthony Vo&#39;s College Notes</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon"></span>
        </label>

        <div class="nav-items">
  <a class="nav-item" href="/college-notes/schedule/">Class Schedule</a>
  <a class="nav-item" href="/college-notes/labs/">Labs</a>
  <a class="nav-item" href="/college-notes/categories/">Tags</a>
  <a class="nav-item" href="/college-notes/search/">Search</a>
</div>

      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Virtual Reality</h1><p class="page-description">COMPSCI 118</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2025-04-02T00:00:00-05:00" itemprop="datePublished">
        Apr 2, 2025
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/college-notes/categories/#cs118">cs118</a>
        &nbsp;
      
        <a class="category-tags-link" href="/college-notes/categories/#lecture">lecture</a>
        &nbsp;
      
        <a class="category-tags-link" href="/college-notes/categories/#spring25">spring25</a>
        &nbsp;
      
        <a class="category-tags-link" href="/college-notes/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h1"><a href="#rendering">Rendering</a></li>
<li class="toc-entry toc-h1"><a href="#graphics-pipeline">Graphics Pipeline</a>
<ul>
<li class="toc-entry toc-h2"><a href="#vertex-procesing">Vertex Procesing</a></li>
<li class="toc-entry toc-h2"><a href="#rasterizer">Rasterizer</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#vr-display">VR Display</a></li>
<li class="toc-entry toc-h1"><a href="#stereo-display">Stereo Display</a>
<ul>
<li class="toc-entry toc-h2"><a href="#hmd-display">HMD Display</a></li>
<li class="toc-entry toc-h2"><a href="#varifocal-display">Varifocal Display</a></li>
</ul>
</li>
</ul><h1 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h1>
<ul>
  <li>
<strong>Formal definition</strong>: Using <em>targeted behavior</em> in an <em>organism</em> using <em>artificial sensory simulation</em> with little or <em>no awareness</em> of the interference on the part of the organism
    <ul>
      <li>
<em>Targeted behavior</em>: A man-made experience</li>
      <li>
<em>Organism</em>: Any organism, not just human</li>
      <li>
<em>Targeted behavior</em>: One or more senses are “taken over” (at least partially) by the virtual world</li>
      <li>
<em>No awareness</em>: “Fooled” to feel like the real world; sense of presence</li>
      <li>Music, movies, and paintings can be thought of as “virtual reality” through this definition</li>
    </ul>
  </li>
  <li>Defined by Immanuel Kant as the reality in someone’s mind</li>
  <li>Jaron Lanier also defined a real world (the physical world) and a virtual world (the perceived world)</li>
  <li>Different terms for VR: Augmented reality (AR), mixed reality (MR), XR, telepresence, teleoperation</li>
  <li>
<strong>Open Loop vs. Closed Loop</strong>: Open loop systems don’t allow for the user to interact, while closed loop systems do</li>
  <li>Components
    <ul>
      <li>Tracking: Input from user, looks at hand, head, body, etc. movements</li>
      <li>Software: Renders and controls the virtual world
        <ul>
          <li>Maintains consistency between real world and virtual world</li>
          <li>Matched zone; users should be able to walk in the real world to walk in the virtual one</li>
          <li>
        </ul>
      </li>
      <li>Display: Outputs the virtual world to the user</li>
      <li>The computer links all these things together</li>
    </ul>
  </li>
  <li>A VR headset uses two different images for your two eyes in order to create the illusion of depth
    <ul>
      <li>Instead of obscuring all other vision, AR uses pass-through monitors as lenses in order to project virtual objects onto the real world</li>
      <li>SAR wants to get rid of any wearables (i.e. headsets) and allow for seamless merging between the virtual and real worlds</li>
    </ul>
  </li>
  <li>Some challenges with VR headsets
    <ul>
      <li>
<strong>Vergence</strong>: Headsets will cannot emulate aspects of depth; eyes will try to focus on something far away, but the screen will stay the same, causing discomfort to the user</li>
      <li>
<strong>Law of Weber and Stephen</strong>: Users will be able to physically feel a difference depending on their stimulus
        <ul>
          <li>$P=KS^n$, where $K = \frac{\text{Difference Threshold}}{\text{Standard Weight}}$ is the Weber fraction, $P$ is the perception, and $S$ is the stimulus strength</li>
          <li>If $n&gt;1$, then we have expansion; if $n&lt;1$, we have compression
            <ul>
              <li>Electric shocks follow expansion (double the shock is more than double the pain), whereas brightnesss follows compression (double the light is less than double the brightness)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<strong>McGurk Effect</strong>: If the lip sync and audio are different, you hear something different</li>
    </ul>
  </li>
  <li>Early VR headsets included stereoscopes, HMD, Nintendo’s Virtual Boy</li>
</ul>

<h1 id="rendering">
<a class="anchor" href="#rendering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rendering</h1>

<ul>
  <li>Has multiple inputs
    <ul>
      <li>3D world: objects, lights, materials, textures</li>
      <li>Camera location, orientation, FoV</li>
    </ul>
  </li>
  <li>Output: 2D image of the world from the camera</li>
  <li>Graphics Pipeline
    <ul>
      <li>Modeling: Coordinate system and objects</li>
      <li>Viewing: Camera/eye, gets rid of objects not being seen</li>
      <li>Illumination and shading</li>
      <li>
<em>Rasterization</em>: Creating a 2D image from the 3D world</li>
      <li>Texture mapping</li>
    </ul>
  </li>
  <li>
<strong>Triangle Soup Model</strong>
    <ul>
      <li>Vertices have a number of attributes, such as coordinates, colors, normals
        <ul>
          <li>
<em>Normals</em> define the direction a face is oriented; can be calculated by averaging the normals of the nearby vertices</li>
        </ul>
      </li>
      <li>Triangles are defined as objects that connect vertices</li>
    </ul>
  </li>
  <li>Techniques
    <ul>
      <li>
<strong>Rasterization</strong>: Project vertices from 3D onto 2D space and draw triangles between them to represent the polygons; done by the GPU</li>
      <li>
<strong>Interpolation</strong>: Automatically generating transitions between colors, frames, polygons, etc.
        <ul>
          <li>Creates interpolation coefficients by averaging out the colors/normals of nearby vertices</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Transformations
    <ul>
      <li>
<strong>Scaling</strong>: Apply a scaling matrix, defined as $S(s_x, s_y, s_z)$ onto a point to transform it
        <ul>
          <li>Matrix has parameters on the diagonal; can be reversed using the inverse matrix, which is equivalent to $S(1/s_x, 1/s_y, 1/s_z)$</li>
        </ul>
      </li>
      <li>
<strong>Rotation</strong>: Apply a rotation matrix which rotates a point about one of the three axes using sine and cosine</li>
      <li>
<strong>Translation</strong>: Must use a 4D matrix and convert the 3-vector into a 4-vector
        <ul>
          <li>
<strong>Homogenous coordinates</strong>: A 3-vector and 4-vector representing the same point in 3D; append an extra 1</li>
          <li>All of the previous transformations can be converted into 4D matrices in order to work with the homogenous representation</li>
        </ul>
      </li>
      <li>
<strong>Shearing</strong>: Translating an object about two out of the three axes by a value proportional to the third axis; affects shape of the object</li>
    </ul>
  </li>
  <li>Can concatenate different transformations onto each other to perform complex operations
    <ul>
      <li>Most notable is rotating/scaling about a <strong>fixed point</strong> by translating to the origin, performing the transformation, and inverting the first translation</li>
    </ul>
  </li>
  <li>An <strong>affine transformation</strong> is any transformation using a 4x4 matrix where the last row is 0 0 0 1
    <ul>
      <li>Degree of curve can’t be changed, and parallel lines cannot become intersecting lines</li>
    </ul>
  </li>
  <li>In <strong>projective transformations</strong>, parallel lines can intersect and vice versa; used when rendering using a pinhole camera</li>
</ul>

<h1 id="graphics-pipeline">
<a class="anchor" href="#graphics-pipeline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Graphics Pipeline</h1>

<ul>
  <li>Input: Soup of triangles</li>
  <li>Output: Image from a particular viewpoint, produced in real time
    <ul>
      <li>The output is put into the <strong>frame buffer</strong>, which is updated according to the FPS and sent to the monitor</li>
    </ul>
  </li>
  <li>Steps (done in the GPU)
    <ul>
      <li>
<em>Vertex Processing</em>: Process the vertices and normals
        <ul>
          <li>Performs transformations on points and per vertex lighting</li>
          <li>Transformations include model, view, and projection transforms</li>
        </ul>
      </li>
      <li>
<em>Rasterization</em>: Convert vertices into a set of fragments (triangles)</li>
      <li>
<em>Fragment Processing</em>: Process individual fragments
        <ul>
          <li>Performs texturing and per fragment lighting</li>
        </ul>
      </li>
      <li>
<em>Output Merging</em>: Combine 3D fragments into 2D space for the display</li>
    </ul>
  </li>
</ul>

<h2 id="vertex-procesing">
<a class="anchor" href="#vertex-procesing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vertex Procesing</h2>

<ul>
  <li>
<em>Model Transform</em>: Begins by arranging the objects in the world using a model transform
    <ul>
      <li>Involves scaling, rotation, translation, shear transformations to propagate the world space</li>
    </ul>
  </li>
  <li>
<em>View Transform</em>: Positions and orients the camera using a view transform
    <ul>
      <li>Translate the camera to the origin ($T$) and then rotate it appropriately ($R$); final transformation is $M = RT$</li>
    </ul>
  </li>
  <li>
<em>Projection Transform</em>: Defines properties of the camera (FOV, lens) and projects the 3D space onto the camera using a projection transform
    <ul>
      <li>Uses gaze direction (shear), FOV, aspect ratio, near plane (image plane), and far plane (cuts off rest of scene) to create the 2D image</li>
      <li>Displays all objects inside of the <strong>view frustum</strong> which is a 3D object connecting the near and far planes
        <ul>
          <li>Must be normalized (to a cube) so conversion to window coordinates is easy</li>
          <li>Requires shear, scale, and projective transformation to convert</li>
        </ul>
      </li>
      <li>Final transformation matrix: ${v_{clip} = M_{proj} \cdot M_{view} \cdot M_{model} \cdot v}$</li>
      <li>Must clip objects to fit on screen by transforming coordinates again</li>
      <li>z-coordinate is retained for occlusion</li>
      <li>To make it look like the camera is coming from the right perspective, a <em>perspective projection</em> is applied which uses translation (to move the camera), shear (to change the LookAt value), and scaling (to convert to cuboid)</li>
    </ul>
  </li>
  <li>A Viewport Transform is performed
    <ul>
      <li>Uses translation to move the nearplane to the center of the window and scaling to scale it to the right size</li>
    </ul>
  </li>
  <li>TLDR: Takes 3D vertices and puts them on a 2D screen, ensuring that only vertices that are “on-screen” are rendered</li>
</ul>

<h2 id="rasterizer">
<a class="anchor" href="#rasterizer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rasterizer</h2>

<ul>
  <li>After clipping and retriangulating, the rasterizer “fills in” the interiors of triangles
    <ul>
      <li>Main issue: Edges of triangles are non-integer, but pixels on a screen are integer; must interpolate vertex attributes onto the pixels</li>
    </ul>
  </li>
  <li>One strategy is to use <strong>scanline interpolation</strong> which involves sweeping across the 2D plane to determine which pixels fall inside of a triangle and interpolating the pixel accordingly
    <ul>
      <li>The z-coordinate allows the rasterizer to figure out which shapes take precedence over others via the <em>depth buffer</em>; known as <strong>occlusion resolution</strong>
</li>
    </ul>
  </li>
  <li>The rasterizer also performs lighting and shading which is extremely difficult to model due to the vast amount of light sources (direct + indirect illumination)</li>
  <li>Lighting
    <ul>
      <li>A simple model would be to remove indirect lighting and to replace with one lighting term</li>
      <li>
<strong>Phong illumination/lighting</strong> calculates three channels (ambient, diffuse, specular) and combinews them to light objects
        <ul>
          <li>Requires a material color and a light color for each channel</li>
          <li>Ambient: viewer-independent, acts as a “background color”, approximates indirect illumniation
            <ul>
              <li>Formula: $m\cdot l$; does not depend on viewer, normals, or light</li>
            </ul>
          </li>
          <li>Diffuse: light coming off of a surface, relies on the angle of lighting and the normal of the surface, approximates some aspects of direct illumination
            <ul>
              <li>Formula: $m\cdot l \cdot \max(L\cdot N, 0)$, where $L\cdot N$ is the dot product between the light and the normal angles</li>
            </ul>
          </li>
          <li>Specular: Reflected light depending on where the viewer is standing, models the shininess of objects, approximates some aspects of direct illumination
            <ul>
              <li>Formula: $m\cdot l \cdot \max(R\cdot V, 0)^{shininess}$, where $R\cdot V$ is the dot product of the refelector and viewer angles</li>
              <li>$shininess$ is an additional parameter required for the specular channel</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<strong>Attentuation</strong> is used to model the falloff of light intensity w.r.t. distance
        <ul>
          <li>Formula for attentuation coefficient: $\frac{1}{k_c + k_ld_i + k_qd_i^2}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Shading
    <ul>
      <li>Shading is the acutal computation of the color for each pixel/fragment/vertex whereas lighting gives the model to do so</li>
      <li>
<strong>Flat shading</strong>: Compute color once per triangle using some model (like Phong lighting)
        <ul>
          <li>Fast to compute, but looks very unrealistic</li>
        </ul>
      </li>
      <li>
<strong>Gourand shading</strong>: Compute color once per vertex and interpolate these colors to the triangles
        <ul>
          <li>A little slower than flat shading, but looks a little more realistic</li>
        </ul>
      </li>
      <li>
<strong>Phong shading</strong>: Compute color once per fragment which requires interpolation of per-vertex normals
        <ul>
          <li>Most realistic, but slowest strategy</li>
        </ul>
      </li>
      <li>Vertex shading will be done before the rasterizer while fragment shading will be done afterwards</li>
    </ul>
  </li>
  <li>Texture mapping is also done in the rasterizer, and the main operation is to map coordinates from the 3D surface (x, y, z) into 2D texture coordinates (u, v)
    <ul>
      <li>These coordinates are interpolated for each fragment</li>
      <li>Since texture coordinates are not guaranteed to be integer, we use <em>texture filtering</em> and use bilinear interpolation</li>
    </ul>
  </li>
</ul>

<h1 id="vr-display">
<a class="anchor" href="#vr-display" aria-hidden="true"><span class="octicon octicon-link"></span></a>VR Display</h1>

<ul>
  <li>Various issues with having a high quality display
    <ul>
      <li>
<em>Visual acuity</em>: VR world must be precise, like reality - should be able to see 15 pixels per inch from 20 feet away
        <ul>
          <li>Varies over different eyes</li>
        </ul>
      </li>
      <li>
<em>Visual field</em>: Seeing with one eye vs. two eyes means that the FOOV should be different</li>
      <li>
<em>Temporal resolution</em>: Video should be smooth in order to not be offputting</li>
      <li>
<em>Depth</em>: Depth is hard to work with; many issues exist like disparity, vergence, accomodation, blur</li>
    </ul>
  </li>
  <li>Biology of vision
    <ul>
      <li>Eye peforms low-level processing, brain does high-level</li>
      <li>Most of the eye’s functions are concentrated within a small part of the eye</li>
      <li>Eye uses cones and rods to process color/light; short, medium, and long wave cones process RGB colors</li>
      <li>Each eye has its own <strong>monocular</strong> visual field that it can see
        <ul>
          <li>The <strong>binocular (stereo) visual field</strong> is the overlap of the two eyes; allows you to see depth</li>
        </ul>
      </li>
      <li>Monocular = <em>periphery</em>, binocular = <em>fovea</em>
        <ul>
          <li>Can only see color in fovea</li>
        </ul>
      </li>
      <li>The total visual field is about 200 degrees; the binocular visual field is about 120 degrees</li>
    </ul>
  </li>
  <li>Headsets are limited because there is a minimum distance that people are able to focus at
    <ul>
      <li>Strategy: Use two lenses, increasing the FOV and weight</li>
    </ul>
  </li>
  <li>Visual acuity is difficult to achieve because the photoreceptors capture 1 arcmin of visual angle
    <ul>
      <li>Leads to requiring a massive amount of pixels; high compute and data requirements</li>
      <li>The eye will see a certain number of “cycles” in one degree, where there are two pixels per cycle (high and low)
        <ul>
          <li>This is known as the <strong>resolution</strong>, more cycles per degree means better resolution</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Minimum Angle of Resolution</strong>: $\omega = me + \omega_0$
    <ul>
      <li>$\omega$: resolution in degrees per cycle</li>
      <li>$e$: angle per eccentricity in degrees</li>
      <li>$\omega_0$: smallest resolvable angle in fovea in degrees per cycle</li>
    </ul>
  </li>
  <li>Visual acuity can be calculated as the reciprocal of MAR ($\omega$)
    <ul>
      <li>Use MAR to accomplish <strong>foveated rendering</strong>: Split the image into multiple layers (inner/foveal, middle, outer) based on MAR and render them as more/less blurry
        <ul>
          <li>Less compute since less pixels have to be rendered; great speedup</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Depth can be perceived in both binocular and monocular vision
    <ul>
      <li>Monocular: Accomodation, retinal blue, motion parallax</li>
      <li>Binocular: (Con)vergence, disparity</li>
      <li>Pictorial cues: Shading, perspective, texture</li>
    </ul>
  </li>
  <li>
<strong>Vergence</strong>: Convergence of muscles to fixate on a single object</li>
  <li>
<strong>Accomodation</strong>: Ability of lens to focus on fixated object</li>
  <li>
<strong>Vergence-Accomodation Conflict</strong>: Eyes try to focus on the screen in front of them, but the screen “fools” them into perceiving depth, creating a difference in vergence and accomodation; causes much fatigure</li>
  <li>
<strong>Motion parallax</strong>: Ability to see different objects while moving</li>
  <li>
<strong>Retinal blur</strong>: Blurring of objects when eyes focus on something different
    <ul>
      <li>Blur can be calculated using $c_z = ar\left(\frac{1}{f} - \frac{1}{z}\right)$
        <ul>
          <li>$r$: Distance to sensor</li>
          <li>$a$: Aperture, controlled by pupil (such as by squinting)</li>
          <li>$f$: Focal length, controlled by accomodation</li>
          <li>$z$: The depth, $d$, that is focused on</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="stereo-display">
<a class="anchor" href="#stereo-display" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stereo Display</h1>

<ul>
  <li>One way to achieve stereo vision using 2D is through glasses
    <ul>
      <li>Can use anaglyph, polarization, etc.</li>
      <li>General strategy: have each eye see a different color/polarization/etc., forcing them to work together to see a 3D image</li>
      <li>
<em>Polarization</em>: Each eye sees different rows + columns, i.e. right eye sees even while left eye sees odd (different polarizations)
        <ul>
          <li>Popular example is RealD</li>
          <li>Inexpensive and makes gaze direction irrelevant</li>
          <li>Screen must be polarization-preserving, and this strategy loses brightness + resolution</li>
        </ul>
      </li>
      <li>
<em>Shutter</em>: Each lens opens and closes exactly when the frame changes
        <ul>
          <li>Somewhat expensive, requires fast display, screen must be synced with glasses</li>
          <li>Active glasses</li>
        </ul>
      </li>
      <li>
<em>Chromatic Filters</em>: Uses two projectors to project two different colored images (one for each eye)
        <ul>
          <li>Somewhat expensive, can’t use in theaters</li>
        </ul>
      </li>
      <li>
<em>Anaglyph</em>: Render stereo images in different colors so that each eye sees a different image
        <ul>
          <li>Most inexpensive glasses</li>
          <li>Has issues with colors</li>
          <li>To create a full-color anaglyph image, render a left and right image then color left using red channel and right using green + blue (red-cyan anaglyph)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Parallax</strong>: The relative distance of a 3D point projected into the 2 stereo images
    <ul>
      <li>Positive parallax means point is behind projection plane, zero parallax means point is on the plane, negative means point is in front</li>
      <li>Must use horizontal parallax where both eyes have the same projection plane (screen), known as <em>off-axis</em> projection
        <ul>
          <li>Projection previously was only from one viewpoint, but now that there are two, we have to account for the horizontal parallax</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="hmd-display">
<a class="anchor" href="#hmd-display" aria-hidden="true"><span class="octicon octicon-link"></span></a>HMD Display</h2>

<ul>
  <li>Basic idea: Have a lens that is a short distance away from a micro display</li>
  <li>Lens magnifies the virtual image to appear at a realistic distance away from the viewer</li>
  <li>Must account for <em>pincushion</em> (inwards) or <em>barrel</em> (outwards) distortion created by the lens by applying the opposite distortion</li>
</ul>

<h2 id="varifocal-display">
<a class="anchor" href="#varifocal-display" aria-hidden="true"><span class="octicon octicon-link"></span></a>Varifocal Display</h2>

<ul>
  <li>Use an actuator and an eye-tracking camera to move the screen away/towards the viewer based on where they are looking in order to remove the vergence-accomodation issue
    <ul>
      <li>Instead of an actuator that moves the screen, a tunable lens with varying focus can be employed to change the distance; more expensive but less technically complicated</li>
    </ul>
  </li>
</ul>

  </div><a class="u-url" href="/college-notes/cs118/lecture/spring25/markdown/2025/04/02/cs118-lecture.html" hidden></a>
</article>

      </div>
    </main><link id="fa-stylesheet" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">

<footer class="site-footer h-card">
  <data class="u-url" value="/college-notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Site to document notes for various classes</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
    <a rel="me" href="" target="_blank" title="">
      <span class="grey fa-brands fa- fa-lg"></span>
    </a>
  </li><li>
    <a rel="me" href="" target="_blank" title="">
      <span class="grey fa-brands fa- fa-lg"></span>
    </a>
  </li>
  <li>
    <a href="https://tonyhieu.github.io/college-notes/feed.xml" target="_blank" title="Subscribe to syndication feed">
      <svg class="svg-icon grey" viewbox="0 0 16 16">
        <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
          11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
          13.806c0-1.21.983-2.195 2.194-2.195zM10.606
          16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
        />
      </svg>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
